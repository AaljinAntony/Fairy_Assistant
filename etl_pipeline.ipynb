{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "9d74ef06",
            "metadata": {},
            "source": [
                "# LLM Contextual ETL + RAG Pipeline\n",
                "\n",
                "This ETL pipeline is designed to ingest unstructured documents and prepare\n",
                "LLM-ready context for a personal AI assistant using retrieval-augmented generation (RAG).\n",
                "\n",
                "## Source Code\n",
                "The source code for this ETL demo is also maintained on GitHub:\n",
                "https://github.com/AaljinAntony/Fairy_Assistant\n",
                "\n",
                "\n",
                "**Pipeline Stages:**\n",
                "1.  **Extract**: Load unstructured text data.\n",
                "2.  **Transform**: Clean and chunk the text into manageable pieces.\n",
                "3.  **Load**: Generate vector embeddings and store them in a vector database.\n",
                "4.  **Retrieval Test**: Verify the pipeline by querying the database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b8ce5f92",
            "metadata": {},
            "outputs": [
                {
                    "ename": "",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[1;31mRunning cells with 'Python 3.12.3' requires the ipykernel package.\n",
                        "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
                    ]
                }
            ],
            "source": [
                "# @title Setup & Installation\n",
                "# Install necessary libraries\n",
                "!pip install sentence-transformers chromadb numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c0b1f724",
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import chromadb\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import re\n",
                "from typing import List, Dict"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "28fa4940",
            "metadata": {},
            "source": [
                "## 1. Extract\n",
                "\n",
                "In a real-world scenario, this stage would involve parsing PDFs, scraping websites, or connecting to APIs. For this demonstration, we use a sample raw text simulating a technical article."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "76db086f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample Raw Text Data (Simulating a technical document)\n",
                "raw_text = \"\"\"\n",
                "Deep learning is a subset of machine learning, which is essentially a neural network with three or more layers.\n",
                "These neural networks attempt to simulate the behavior of the human brain—albeit far from matching its ability—allowing it to \"learn\" from large amounts of data.\n",
                "While a neural network with a single layer can still make approximate predictions, additional hidden layers can help to optimize and refine for accuracy.\n",
                "\n",
                "Deep learning drives many artificial intelligence (AI) applications and services that improve automation, performing analytical and physical tasks without human intervention.\n",
                "Deep learning technology lies behind everyday products and services (such as digital assistants, voice-enabled TV remotes, and credit card fraud detection) as well as emerging technologies (such as self-driving cars).\n",
                "\n",
                "RAG (Retrieval-Augmented Generation) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources.\n",
                "By constructing a prompt that includes both the user's query and relevant retrieved data, RAG allows the model to generate responses that are grounded in specific, up-to-date information.\n",
                "\"\"\"\n",
                "\n",
                "print(f\"Extracted {len(raw_text)} characters of raw text.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "ea4ba89a",
            "metadata": {},
            "source": [
                "## 2. Transform (Cleaning & Chunking)\n",
                "\n",
                "Context windows in LLMs are limited. We must split (chunk) the text into smaller segments. We use **recursive chunking**, which tries to split by meaningful separators (paragraphs, newlines, sentences) to preserve semantic context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c56a5d91",
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text: str) -> str:\n",
                "    \"\"\"Basic text cleaning to normalize whitespace.\"\"\"\n",
                "    # Replace multiple newlines with a single newline marker for structure, or just space\n",
                "    # Here we perform simple space normalization\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    return text\n",
                "\n",
                "def recursive_chunker(text: str, chunk_size: int = 150, chunk_overlap: int = 20) -> List[str]:\n",
                "    \"\"\"\n",
                "    Splits text into chunks of approximately `chunk_size` characters.\n",
                "    Prioritizes splitting by separators to keep sentences intact.\n",
                "    Ensures splits happen at word boundaries.\n",
                "    \"\"\"\n",
                "    separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
                "    chunks = []\n",
                "    start = 0\n",
                "    \n",
                "    while start < len(text):\n",
                "        end = start + chunk_size\n",
                "        if end >= len(text):\n",
                "            chunks.append(text[start:])\n",
                "            break\n",
                "            \n",
                "        # Find the best separator near the end of the chunk\n",
                "        best_split = end\n",
                "        found_separator = False\n",
                "        \n",
                "        # Look backwards from 'end' to find a safe split point\n",
                "        for sep in separators:\n",
                "            if sep == \"\": continue # fallback\n",
                "            # Search only within the last 50% of the chunk to avoid too small chunks\n",
                "            search_start = max(start, end - chunk_size // 2)\n",
                "            # Find separator using rfind\n",
                "            split_idx = text.rfind(sep, search_start, end)\n",
                "            \n",
                "            if split_idx != -1:\n",
                "                best_split = split_idx + len(sep)\n",
                "                found_separator = True\n",
                "                break\n",
                "        \n",
                "        # If no separator found, try to find the nearest space to avoid cutting words\n",
                "        if not found_separator:\n",
                "             # Look primarily for space\n",
                "             space_idx = text.rfind(\" \", max(start, end - 50), end)\n",
                "             if space_idx != -1:\n",
                "                 best_split = space_idx + 1\n",
                "        \n",
                "        chunk = text[start:best_split]\n",
                "        chunks.append(chunk.strip())\n",
                "        \n",
                "        # Calculate next start position with overlap\n",
                "        next_start = best_split - chunk_overlap\n",
                "        \n",
                "        # Adjust next_start to align with a word boundary (space)\n",
                "        # We search backwards from next_start to find the nearest space\n",
                "        if next_start > 0 and next_start < len(text):\n",
                "            space_idx = text.rfind(\" \", max(0, next_start - 20), next_start)\n",
                "            if space_idx != -1:\n",
                "                next_start = space_idx + 1\n",
                "                \n",
                "        # Ensure valid start for next loop\n",
                "        start = max(start + 1, next_start)\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "# Execution\n",
                "cleaned_text = clean_text(raw_text)\n",
                "chunks = recursive_chunker(cleaned_text, chunk_size=200, chunk_overlap=30)\n",
                "\n",
                "print(f\"Generated {len(chunks)} chunks:\")\n",
                "for i, c in enumerate(chunks):\n",
                "    print(f\"Chunk {i+1}: {c[:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load (Embeddings & Vector DB)\n",
                "\n",
                "We convert text chunks into numerical vectors (embeddings) using a pre-trained model (`all-MiniLM-L6-v2`) and store them in **ChromaDB**, an open-source vector database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Initialize Embedding Model\n",
                "print(\"Loading embedding model...\")\n",
                "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "\n",
                "# 2. Generate Embeddings\n",
                "embeddings = model.encode(chunks)\n",
                "print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
                "\n",
                "# 3. Initialize ChromaDB (Ephemeral - In-memory)\n",
                "client = chromadb.Client()\n",
                "\n",
                "# Handle case where collection already exists (for re-running notebook)\n",
                "try:\n",
                "    client.delete_collection(name=\"demo_knowledge_base\")\n",
                "except Exception:\n",
                "    pass\n",
                "\n",
                "collection = client.create_collection(name=\"demo_knowledge_base\")\n",
                "\n",
                "# 4. Load Data into Vector DB\n",
                "ids = [f\"id_{i}\" for i in range(len(chunks))]\n",
                "metadatas = [{\"source\": \"sample_text\", \"chunk_index\": i} for i in range(len(chunks))]\n",
                "\n",
                "collection.add(\n",
                "    documents=chunks,\n",
                "    embeddings=embeddings.tolist(),\n",
                "    metadatas=metadatas,\n",
                "    ids=ids\n",
                ")\n",
                "\n",
                "print(f\"Successfully loaded {len(chunks)} items into ChromaDB.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Retrieval Test\n",
                "\n",
                "We verify the pipeline by simulating a user query. The system embeds the query and finds the most similar chunks (Nearest Neighbors)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"What is RAG?\"\n",
                "\n",
                "# 1. Embed Query\n",
                "query_embedding = model.encode([query]).tolist()\n",
                "\n",
                "# 2. Search Database\n",
                "results = collection.query(\n",
                "    query_embeddings=query_embedding,\n",
                "    n_results=2\n",
                ")\n",
                "\n",
                "# 3. Display Results\n",
                "print(f\"Query: '{query}'\\n\")\n",
                "print(\"Top Retrieved Results:\")\n",
                "for i, doc in enumerate(results['documents'][0]):\n",
                "    print(f\"--- Result {i+1} ---\")\n",
                "    print(f\"Text: {doc}\")\n",
                "    print(f\"Distance: {results['distances'][0][i]:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
