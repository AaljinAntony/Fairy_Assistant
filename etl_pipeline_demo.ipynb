{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# End-to-End ETL Pipeline for LLM Context Ingestion\n",
                "\n",
                "This notebook demonstrates a minimal but complete ETL (Extract, Transform, Load) pipeline for preparing text data for RAG (Retrieval-Augmented Generation) systems.\n",
                "\n",
                "**Pipeline Stages:**\n",
                "1.  **Extract**: Load unstructured text data.\n",
                "2.  **Transform**: Clean and chunk the text into manageable pieces.\n",
                "3.  **Load**: Generate vector embeddings and store them in a vector database.\n",
                "4.  **Retrieval Test**: Verify the pipeline by querying the database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# @title Setup & Installation\n",
                "# Install necessary libraries\n",
                "!pip install sentence-transformers chromadb numpy"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import chromadb\n",
                "from sentence_transformers import SentenceTransformer\n",
                "import re\n",
                "from typing import List, Dict"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Extract\n",
                "\n",
                "In a real-world scenario, this stage would involve parsing PDFs, scraping websites, or connecting to APIs. For this demonstration, we use a sample raw text simulating a technical article."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Sample Raw Text Data (Simulating a technical document)\n",
                "raw_text = \"\"\"\n",
                "Deep learning is a subset of machine learning, which is essentially a neural network with three or more layers.\n",
                "These neural networks attempt to simulate the behavior of the human brain—albeit far from matching its ability—allowing it to \"learn\" from large amounts of data.\n",
                "While a neural network with a single layer can still make approximate predictions, additional hidden layers can help to optimize and refine for accuracy.\n",
                "\n",
                "Deep learning drives many artificial intelligence (AI) applications and services that improve automation, performing analytical and physical tasks without human intervention.\n",
                "Deep learning technology lies behind everyday products and services (such as digital assistants, voice-enabled TV remotes, and credit card fraud detection) as well as emerging technologies (such as self-driving cars).\n",
                "\n",
                "RAG (Retrieval-Augmented Generation) is a technique for enhancing the accuracy and reliability of generative AI models with facts fetched from external sources.\n",
                "By constructing a prompt that includes both the user's query and relevant retrieved data, RAG allows the model to generate responses that are grounded in specific, up-to-date information.\n",
                "\"\"\"\n",
                "\n",
                "print(f\"Extracted {len(raw_text)} characters of raw text.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Transform (Cleaning & Chunking)\n",
                "\n",
                "Context windows in LLMs are limited. We must split (chunk) the text into smaller segments. We use **recursive chunking**, which tries to split by meaningful separators (paragraphs, newlines, sentences) to preserve semantic context."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_text(text: str) -> str:\n",
                "    \"\"\"Basic text cleaning to normalize whitespace.\"\"\"\n",
                "    # Replace multiple newlines with a single newline marker for structure, or just space\n",
                "    # Here we perform simple space normalization\n",
                "    text = re.sub(r'\\s+', ' ', text).strip()\n",
                "    return text\n",
                "\n",
                "def recursive_chunker(text: str, chunk_size: int = 150, chunk_overlap: int = 20) -> List[str]:\n",
                "    \"\"\"\n",
                "    Splits text into chunks of approximately `chunk_size` characters.\n",
                "    Prioritizes splitting by separators to keep sentences intact.\n",
                "    \"\"\"\n",
                "    separators = [\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
                "    chunks = []\n",
                "    start = 0\n",
                "    \n",
                "    while start < len(text):\n",
                "        end = start + chunk_size\n",
                "        if end >= len(text):\n",
                "            chunks.append(text[start:])\n",
                "            break\n",
                "            \n",
                "        # Find the best separator near the end of the chunk\n",
                "        best_split = end\n",
                "        found_separator = False\n",
                "        \n",
                "        # Look backwards from 'end' to find a safe split point\n",
                "        for sep in separators:\n",
                "            if sep == \"\": continue # fallback\n",
                "            # Search only within the last 50% of the chunk to avoid too small chunks\n",
                "            search_start = max(start, end - chunk_size // 2)\n",
                "            # Find separator using rfind\n",
                "            split_idx = text.rfind(sep, search_start, end)\n",
                "            \n",
                "            if split_idx != -1:\n",
                "                best_split = split_idx + len(sep)\n",
                "                found_separator = True\n",
                "                break\n",
                "        \n",
                "        # If no separator found, just cut at 'end'\n",
                "        chunk = text[start:best_split]\n",
                "        chunks.append(chunk.strip())\n",
                "        \n",
                "        # Move start forward, respecting overlap\n",
                "        start = best_split - chunk_overlap\n",
                "    \n",
                "    return chunks\n",
                "\n",
                "# Execution\n",
                "cleaned_text = clean_text(raw_text)\n",
                "chunks = recursive_chunker(cleaned_text, chunk_size=200, chunk_overlap=30)\n",
                "\n",
                "print(f\"Generated {len(chunks)} chunks:\")\n",
                "for i, c in enumerate(chunks):\n",
                "    print(f\"Chunk {i+1}: {c[:50]}...\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Load (Embeddings & Vector DB)\n",
                "\n",
                "We convert text chunks into numerical vectors (embeddings) using a pre-trained model (`all-MiniLM-L6-v2`) and store them in **ChromaDB**, an open-source vector database."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 1. Initialize Embedding Model\n",
                "print(\"Loading embedding model...\")\n",
                "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
                "\n",
                "# 2. Generate Embeddings\n",
                "embeddings = model.encode(chunks)\n",
                "print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
                "\n",
                "# 3. Initialize ChromaDB (Ephemeral - In-memory)\n",
                "client = chromadb.Client()\n",
                "collection = client.create_collection(name=\"demo_knowledge_base\")\n",
                "\n",
                "# 4. Load Data into Vector DB\n",
                "ids = [f\"id_{i}\" for i in range(len(chunks))]\n",
                "metadatas = [{\"source\": \"sample_text\", \"chunk_index\": i} for i in range(len(chunks))]\n",
                "\n",
                "collection.add(\n",
                "    documents=chunks,\n",
                "    embeddings=embeddings.tolist(),\n",
                "    metadatas=metadatas,\n",
                "    ids=ids\n",
                ")\n",
                "\n",
                "print(f\"Successfully loaded {len(chunks)} items into ChromaDB.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Retrieval Test\n",
                "\n",
                "We verify the pipeline by simulating a user query. The system embeds the query and finds the most similar chunks (Nearest Neighbors)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "query = \"What is RAG?\"\n",
                "\n",
                "# 1. Embed Query\n",
                "query_embedding = model.encode([query]).tolist()\n",
                "\n",
                "# 2. Search Database\n",
                "results = collection.query(\n",
                "    query_embeddings=query_embedding,\n",
                "    n_results=2\n",
                ")\n",
                "\n",
                "# 3. Display Results\n",
                "print(f\"Query: '{query}'\\n\")\n",
                "print(\"Top Retrieved Results:\")\n",
                "for i, doc in enumerate(results['documents'][0]):\n",
                "    print(f\"--- Result {i+1} ---\")\n",
                "    print(f\"Text: {doc}\")\n",
                "    print(f\"Distance: {results['distances'][0][i]:.4f}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}